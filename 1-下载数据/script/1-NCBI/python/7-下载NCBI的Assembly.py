#!/usr/bin/env python3
# coding: utf-8
"""
æ‰¹é‡ä¸‹è½½ NCBI Datasets æ•°æ®ï¼ˆå·²é…ç½®é»˜è®¤é‚®ç®±å’Œ API Keyï¼‰
æ”¯æŒæ–­ç‚¹ç»­è·‘ã€å½©è‰²è¾“å‡ºã€è¿›åº¦æ˜¾ç¤ºå’Œæ¸…ç†æœªå®Œæˆæ–‡ä»¶

ä½¿ç”¨æ–¹æ³•ç¤ºä¾‹ï¼š
  export NCBI_API_KEY="..."      # å¯é€‰ï¼šè¦†ç›–è„šæœ¬å†…é»˜è®¤
  export NCBI_EMAIL="..."        # å¯é€‰ï¼šè¦†ç›–è„šæœ¬å†…é»˜è®¤
  python3 download_datasets.py -b /mnt/c/Users/Administrator/Desktop/ -f /path/to/ä¸‹è½½NCBI.txt -w 5 --resume
"""
import os
import argparse
import requests
from requests.adapters import HTTPAdapter
from urllib3.util.retry import Retry
from concurrent.futures import ThreadPoolExecutor, as_completed
import threading
import time
import signal
import sys
import glob
import json
from datetime import datetime
from urllib.parse import urlparse, parse_qs, urlencode, urlunparse
from pathlib import Path

# å½©è‰²è¾“å‡ºæ”¯æŒ
try:
    from colorama import init, Fore, Style, Back
    init(autoreset=True)
    HAS_COLORAMA = True
except ImportError:
    HAS_COLORAMA = False
    
# è¿›åº¦æ¡æ”¯æŒ
try:
    from tqdm import tqdm
    HAS_TQDM = True
except ImportError:
    HAS_TQDM = False

# -------------------- é»˜è®¤é…ç½®ï¼ˆå¯é€šè¿‡ç¯å¢ƒå˜é‡æˆ–å‘½ä»¤è¡Œè¦†ç›–ï¼‰ --------------------
DEFAULT_NCBI_EMAIL = os.environ.get("NCBI_EMAIL", "giantlinlinlin@gmail.com")
DEFAULT_NCBI_API_KEY = os.environ.get("NCBI_API_KEY", "29b326d54e7a21fc6c8b9afe7d71f441d809")
# -------------------------------------------------------------------------------

# å…¨å±€é€Ÿç‡æ§åˆ¶ï¼ˆä¼šåœ¨ main ä¸­æ ¹æ®æ˜¯å¦æœ‰ key è°ƒæ•´ allowed_rpsï¼‰
RATE_LOCK = threading.Lock()
LAST_REQUEST_TIME = 0.0
MIN_INTERVAL = 0.0  # æ¯æ¬¡è¯·æ±‚æœ€å°é—´éš”ç§’ï¼Œä¹‹ååœ¨ main() ä¸­è®¡ç®—

# å…¨å±€å˜é‡ç”¨äºæ¸…ç†
TEMP_FILES = set()
TEMP_FILES_LOCK = threading.Lock()
SHUTDOWN_EVENT = threading.Event()

# å½©è‰²è¾“å‡ºå‡½æ•°
def colored_print(message, color="white", style="normal"):
    """æ‰“å°å½©è‰²æ¶ˆæ¯"""
    if not HAS_COLORAMA:
        print(message)
        return
    
    color_map = {
        "red": Fore.RED,
        "green": Fore.GREEN,
        "yellow": Fore.YELLOW,
        "blue": Fore.BLUE,
        "magenta": Fore.MAGENTA,
        "cyan": Fore.CYAN,
        "white": Fore.WHITE,
    }
    
    style_map = {
        "normal": Style.NORMAL,
        "bright": Style.BRIGHT,
        "dim": Style.DIM,
    }
    
    color_code = color_map.get(color, Fore.WHITE)
    style_code = style_map.get(style, Style.NORMAL)
    
    print(f"{style_code}{color_code}{message}{Style.RESET_ALL}")

def print_success(message):
    """æ‰“å°æˆåŠŸæ¶ˆæ¯"""
    colored_print(f"âœ“ {message}", "green", "bright")

def print_error(message):
    """æ‰“å°é”™è¯¯æ¶ˆæ¯"""
    colored_print(f"âœ— {message}", "red", "bright")

def print_warning(message):
    """æ‰“å°è­¦å‘Šæ¶ˆæ¯"""
    colored_print(f"âš  {message}", "yellow", "bright")

def print_info(message):
    """æ‰“å°ä¿¡æ¯æ¶ˆæ¯"""
    colored_print(f"â„¹ {message}", "blue", "bright")

def print_progress(message):
    """æ‰“å°è¿›åº¦æ¶ˆæ¯"""
    colored_print(f"âš¡ {message}", "cyan", "bright")

# ä¸´æ—¶æ–‡ä»¶ç®¡ç†
def add_temp_file(filepath):
    """æ·»åŠ ä¸´æ—¶æ–‡ä»¶åˆ°è·Ÿè¸ªåˆ—è¡¨"""
    with TEMP_FILES_LOCK:
        TEMP_FILES.add(filepath)

def remove_temp_file(filepath):
    """ä»è·Ÿè¸ªåˆ—è¡¨ç§»é™¤ä¸´æ—¶æ–‡ä»¶"""
    with TEMP_FILES_LOCK:
        TEMP_FILES.discard(filepath)

def cleanup_temp_files():
    """æ¸…ç†æ‰€æœ‰ä¸´æ—¶æ–‡ä»¶"""
    with TEMP_FILES_LOCK:
        if TEMP_FILES:
            print_warning(f"æ­£åœ¨æ¸…ç† {len(TEMP_FILES)} ä¸ªæœªå®Œæˆçš„ä¸‹è½½æ–‡ä»¶...")
            for temp_file in TEMP_FILES.copy():
                try:
                    if os.path.exists(temp_file):
                        os.remove(temp_file)
                        print_info(f"å·²åˆ é™¤: {temp_file}")
                except Exception as e:
                    print_error(f"åˆ é™¤æ–‡ä»¶å¤±è´¥ {temp_file}: {e}")
            TEMP_FILES.clear()
            print_success("ä¸´æ—¶æ–‡ä»¶æ¸…ç†å®Œæˆ")

def signal_handler(signum, frame):
    """ä¿¡å·å¤„ç†å™¨ï¼Œç”¨äºä¼˜é›…å…³é—­"""
    print_warning("\næ”¶åˆ°ä¸­æ–­ä¿¡å·ï¼Œæ­£åœ¨ä¼˜é›…å…³é—­...")
    SHUTDOWN_EVENT.set()
    cleanup_temp_files()
    print_info("ç¨‹åºå·²ç»ˆæ­¢")
    sys.exit(0)

# æ–­ç‚¹ç»­è·‘ç›¸å…³å‡½æ•°
def load_progress_state(base_path):
    """åŠ è½½ä¹‹å‰çš„ä¸‹è½½è¿›åº¦"""
    state_file = os.path.join(base_path, "download_progress.json")
    if os.path.exists(state_file):
        try:
            with open(state_file, 'r', encoding='utf-8') as f:
                return json.load(f)
        except Exception as e:
            print_warning(f"æ— æ³•åŠ è½½è¿›åº¦æ–‡ä»¶: {e}")
    return {"completed": set(), "failed": set()}

def save_progress_state(base_path, completed, failed):
    """ä¿å­˜ä¸‹è½½è¿›åº¦"""
    state_file = os.path.join(base_path, "download_progress.json")
    state = {
        "completed": list(completed),
        "failed": list(failed),
        "last_update": datetime.now().isoformat()
    }
    try:
        with open(state_file, 'w', encoding='utf-8') as f:
            json.dump(state, f, indent=2, ensure_ascii=False)
    except Exception as e:
        print_error(f"ä¿å­˜è¿›åº¦å¤±è´¥: {e}")

def check_existing_files(base_path, accession_numbers):
    """æ£€æŸ¥å·²å­˜åœ¨çš„æ–‡ä»¶ï¼Œè¿”å›å·²å®Œæˆçš„accessionåˆ—è¡¨"""
    completed = set()
    for acc in accession_numbers:
        zip_file = os.path.join(base_path, f"{acc}_downloaded.zip")
        if os.path.exists(zip_file) and os.path.getsize(zip_file) > 0:
            completed.add(acc)
    return completed

def parse_args():
    parser = argparse.ArgumentParser(
        description="æ‰¹é‡ä¸‹è½½ NCBI Datasets æ•°æ®",
        formatter_class=argparse.RawDescriptionHelpFormatter,
        epilog="""
ç¤ºä¾‹ç”¨æ³•:
  # é¦–æ¬¡ä¸‹è½½
  python3 %(prog)s -b /path/to/output/ -f accessions.txt -w 5
  
  # æ–­ç‚¹ç»­è·‘
  python3 %(prog)s -b /path/to/output/ -f accessions.txt -w 5 --resume
  
  # å¼ºåˆ¶é‡æ–°ä¸‹è½½æ‰€æœ‰æ–‡ä»¶
  python3 %(prog)s -b /path/to/output/ -f accessions.txt -w 5 --force-redownload
        """
    )
    parser.add_argument(
        "--base-path", "-b",
        required=True,
        help="è¾“å‡ºç»“æœå’Œä¸‹è½½æ–‡ä»¶çš„åŸºç¡€è·¯å¾„ï¼Œä¾‹å¦‚ /mnt/c/Users/Administrator/Desktop/"
    )
    parser.add_argument(
        "--file-path", "-f",
        required=True,
        help="å­˜æ”¾ accession åˆ—è¡¨çš„æ–‡æœ¬æ–‡ä»¶è·¯å¾„ï¼Œä¾‹å¦‚ /path/to/ä¸‹è½½NCBI.txt"
    )
    parser.add_argument(
        "--email", "-e",
        default=DEFAULT_NCBI_EMAIL,
        help=f"è”ç³»é‚®ç®±ï¼ˆé»˜è®¤: {DEFAULT_NCBI_EMAIL}ï¼‰"
    )
    parser.add_argument(
        "--api-key", "-k",
        default=DEFAULT_NCBI_API_KEY,
        help=f"NCBI API Keyï¼ˆé»˜è®¤å·²å†…ç½®ï¼‰"
    )
    parser.add_argument(
        "--workers", "-w",
        type=int,
        default=5,
        help="å¹¶å‘çº¿ç¨‹æ•°ï¼ˆé»˜è®¤5ï¼‰"
    )
    parser.add_argument(
        "--resume", "-r",
        action="store_true",
        help="å¯ç”¨æ–­ç‚¹ç»­è·‘ï¼Œè·³è¿‡å·²ä¸‹è½½çš„æ–‡ä»¶"
    )
    parser.add_argument(
        "--force-redownload",
        action="store_true",
        help="å¼ºåˆ¶é‡æ–°ä¸‹è½½æ‰€æœ‰æ–‡ä»¶ï¼Œå¿½ç•¥å·²å­˜åœ¨çš„æ–‡ä»¶"
    )
    parser.add_argument(
        "--no-color",
        action="store_true",
        help="ç¦ç”¨å½©è‰²è¾“å‡º"
    )
    return parser.parse_args()

def read_accession_file(file_path):
    with open(file_path, 'r', encoding='utf-8') as file:
        return [accession.strip() for accession in file if accession.strip()]

def generate_urls(accession_numbers):
    URL_TEMPLATE = (
        "https://api.ncbi.nlm.nih.gov/datasets/v2/genome/accession/{}/download"
        "?include_annotation_type=GENOME_FASTA"
        "&include_annotation_type=GENOME_GFF"
        "&include_annotation_type=CDS_FASTA"
    )
    return [URL_TEMPLATE.format(acc) for acc in accession_numbers]

def append_api_key_to_url(url, api_key):
    """
    å¦‚æœ api_key éç©ºä¸” URL ä¸­è¿˜æ²¡æœ‰ api_key å‚æ•°ï¼Œåˆ™è¿½åŠ  api_key å‚æ•°å¹¶è¿”å›æ–°çš„ URLã€‚
    """
    if not api_key:
        return url
    parsed = urlparse(url)
    qs = parse_qs(parsed.query)
    if "api_key" in qs and qs["api_key"]:
        return url  # å·²æœ‰ api_keyï¼Œç›´æ¥è¿”å›
    qs["api_key"] = [api_key]
    new_query = urlencode(qs, doseq=True)
    new_parsed = parsed._replace(query=new_query)
    return urlunparse(new_parsed)

def rate_limit_wait():
    """
    å…¨å±€é€Ÿç‡é™åˆ¶ï¼šç¡®ä¿çº¿ç¨‹é—´çš„è¯·æ±‚é—´éš”è‡³å°‘ä¸º MIN_INTERVAL ç§’
    """
    global LAST_REQUEST_TIME
    with RATE_LOCK:
        now = time.time()
        elapsed = now - LAST_REQUEST_TIME
        if elapsed < MIN_INTERVAL:
            to_sleep = MIN_INTERVAL - elapsed
            time.sleep(to_sleep)
            LAST_REQUEST_TIME = time.time()
        else:
            LAST_REQUEST_TIME = now

def create_session(email):
    """
    åˆ›å»ºå…±äº«çš„ requests.Sessionï¼Œå¸¦ retry ç­–ç•¥å’Œå¸¸ç”¨ headersï¼ˆåŒ…å« emailï¼‰
    """
    session = requests.Session()
    retry_strategy = Retry(
        total=5,
        backoff_factor=1,
        status_forcelist=[429, 500, 502, 503, 504],
        allowed_methods=["GET"]
    )
    adapter = HTTPAdapter(max_retries=retry_strategy)
    session.mount("https://", adapter)
    # å°† email æ”¾å…¥ User-Agent å’Œ From headerï¼ˆä¸å°‘æœºæ„å¸Œæœ›çœ‹åˆ° contactï¼‰
    ua = f"ncbi-datasets-downloader ({email})"
    session.headers.update({
        "User-Agent": ua,
        "From": email
    })
    return session

def download_file(session, url, output_filename, api_key, accession):
    """
    ä¸‹è½½å•ä¸ªæ–‡ä»¶ï¼ˆstreamï¼‰ï¼Œåœ¨æ¯æ¬¡è¯·æ±‚å‰åšå…¨å±€é€Ÿç‡é™åˆ¶ï¼Œå¹¶è¿”å›ç»“æœå­—å…¸
    """
    if SHUTDOWN_EVENT.is_set():
        return {"status": "cancelled", "accession": accession, "message": "ä¸‹è½½è¢«ç”¨æˆ·å–æ¶ˆ"}
    
    try:
        final_url = append_api_key_to_url(url, api_key)
        # å…¨å±€é€Ÿç‡é™åˆ¶
        rate_limit_wait()
        
        if SHUTDOWN_EVENT.is_set():
            return {"status": "cancelled", "accession": accession, "message": "ä¸‹è½½è¢«ç”¨æˆ·å–æ¶ˆ"}
        
        # å‘èµ·è¯·æ±‚ï¼ˆæµå¼ï¼‰
        with session.get(final_url, stream=True, timeout=60) as response:
            if response.status_code == 200:
                # ä¸´æ—¶å†™å…¥æ–‡ä»¶ï¼ˆé¿å…åŠæˆªæ–‡ä»¶è¢«è¯¯åˆ¤ä¸ºæˆåŠŸï¼‰
                tmp_out = output_filename + ".part"
                add_temp_file(tmp_out)  # æ·»åŠ åˆ°ä¸´æ—¶æ–‡ä»¶è·Ÿè¸ª
                
                try:
                    # è·å–æ–‡ä»¶å¤§å°ç”¨äºè¿›åº¦æ˜¾ç¤º
                    total_size = int(response.headers.get('content-length', 0))
                    
                    with open(tmp_out, 'wb') as f:
                        downloaded = 0
                        for chunk in response.iter_content(chunk_size=1024*16):
                            if SHUTDOWN_EVENT.is_set():
                                return {"status": "cancelled", "accession": accession, "message": "ä¸‹è½½è¢«ç”¨æˆ·å–æ¶ˆ"}
                            
                            if chunk:
                                f.write(chunk)
                                downloaded += len(chunk)
                    
                    # ä¸‹è½½æˆåŠŸåé‡å‘½åå¹¶ä»ä¸´æ—¶æ–‡ä»¶åˆ—è¡¨ç§»é™¤
                    os.replace(tmp_out, output_filename)
                    remove_temp_file(tmp_out)
                    
                    # æ ¼å¼åŒ–æ–‡ä»¶å¤§å°
                    size_mb = downloaded / (1024 * 1024)
                    return {
                        "status": "success",
                        "accession": accession,
                        "message": f"æˆåŠŸä¸‹è½½ {accession} ({size_mb:.2f} MB)",
                        "filename": output_filename,
                        "size": downloaded
                    }
                    
                except Exception as e:
                    # ä¸‹è½½è¿‡ç¨‹ä¸­å‡ºé”™ï¼Œæ¸…ç†ä¸´æ—¶æ–‡ä»¶
                    if os.path.exists(tmp_out):
                        try:
                            os.remove(tmp_out)
                        except:
                            pass
                    remove_temp_file(tmp_out)
                    raise e
                    
            else:
                return {
                    "status": "failed",
                    "accession": accession,
                    "message": f"HTTPé”™è¯¯ {response.status_code}: {url}",
                    "url": url
                }
                
    except Exception as e:
        return {
            "status": "failed",
            "accession": accession,
            "message": f"ä¸‹è½½å¤±è´¥ {accession}: {str(e)}",
            "url": url
        }

def concurrent_download(urls_and_accessions, base_path, success_file, failure_file, api_key, workers, email, resume=False):
    """
    å¹¶å‘ä¸‹è½½æ–‡ä»¶ï¼Œæ”¯æŒæ–­ç‚¹ç»­è·‘
    urls_and_accessions: [(url, accession), ...] å…ƒç»„åˆ—è¡¨
    """
    os.makedirs(base_path, exist_ok=True)
    session = create_session(email)
    
    # ç»Ÿè®¡ä¿¡æ¯
    stats = {
        "total": len(urls_and_accessions),
        "completed": 0,
        "failed": 0,
        "skipped": 0,
        "cancelled": 0,
        "start_time": time.time(),
        "total_size": 0
    }
    
    # å¦‚æœå¯ç”¨æ–­ç‚¹ç»­è·‘ï¼Œæ£€æŸ¥å·²å®Œæˆçš„æ–‡ä»¶
    completed_accessions = set()
    if resume:
        # ä»è¿›åº¦æ–‡ä»¶åŠ è½½çŠ¶æ€
        progress_state = load_progress_state(base_path)
        completed_accessions = set(progress_state.get("completed", []))
        
        # æ£€æŸ¥å®é™…æ–‡ä»¶å­˜åœ¨æ€§
        existing_files = check_existing_files(base_path, [acc for _, acc in urls_and_accessions])
        completed_accessions.update(existing_files)
        
        if completed_accessions:
            print_info(f"å‘ç° {len(completed_accessions)} ä¸ªå·²å®Œæˆçš„ä¸‹è½½ï¼Œå°†è·³è¿‡")
    
    # è¿‡æ»¤éœ€è¦ä¸‹è½½çš„é¡¹ç›®
    to_download = [(url, acc) for url, acc in urls_and_accessions if acc not in completed_accessions]
    stats["skipped"] = len(urls_and_accessions) - len(to_download)
    
    if stats["skipped"] > 0:
        print_progress(f"è·³è¿‡ {stats['skipped']} ä¸ªå·²å®Œæˆçš„æ–‡ä»¶")
    
    if not to_download:
        print_success("æ‰€æœ‰æ–‡ä»¶éƒ½å·²ä¸‹è½½å®Œæˆ!")
        return stats
    
    print_info(f"å‡†å¤‡ä¸‹è½½ {len(to_download)} ä¸ªæ–‡ä»¶ï¼Œä½¿ç”¨ {workers} ä¸ªå¹¶å‘çº¿ç¨‹")
    
    # åˆ›å»ºè¿›åº¦æ¡ (å¦‚æœå¯ç”¨)
    pbar = None
    if HAS_TQDM and len(to_download) > 1:
        pbar = tqdm(total=len(to_download), desc="ä¸‹è½½è¿›åº¦", 
                   bar_format="{l_bar}{bar}| {n_fmt}/{total_fmt} [{elapsed}<{remaining}, {rate_fmt}]")
    
    with ThreadPoolExecutor(max_workers=workers) as executor, \
         open(success_file, 'a', encoding='utf-8') as sf, \
         open(failure_file, 'a', encoding='utf-8') as ff:
        
        future_to_info = {}
        
        # æäº¤æ‰€æœ‰ä¸‹è½½ä»»åŠ¡
        for url, accession in to_download:
            if SHUTDOWN_EVENT.is_set():
                break
                
            out_fname = os.path.join(base_path, f"{accession}_downloaded.zip")
            future = executor.submit(download_file, session, url, out_fname, api_key, accession)
            future_to_info[future] = {"url": url, "accession": accession}
        
        # å¤„ç†å®Œæˆçš„ä»»åŠ¡
        completed_in_session = set()
        failed_in_session = set()
        
        try:
            for future in as_completed(future_to_info):
                if SHUTDOWN_EVENT.is_set():
                    break
                    
                info = future_to_info[future]
                result = future.result()
                
                if result["status"] == "success":
                    print_success(result["message"])
                    sf.write(f"{datetime.now().isoformat()}: {result['message']}\n")
                    sf.flush()
                    stats["completed"] += 1
                    stats["total_size"] += result.get("size", 0)
                    completed_in_session.add(result["accession"])
                    
                elif result["status"] == "cancelled":
                    print_warning(result["message"])
                    stats["cancelled"] += 1
                    
                else:  # failed
                    print_error(result["message"])
                    ff.write(f"{datetime.now().isoformat()}: {result['message']}\n")
                    ff.flush()
                    stats["failed"] += 1
                    failed_in_session.add(result["accession"])
                
                # æ›´æ–°è¿›åº¦æ¡
                if pbar:
                    pbar.update(1)
                    pbar.set_postfix({
                        "æˆåŠŸ": stats["completed"],
                        "å¤±è´¥": stats["failed"],
                        "å–æ¶ˆ": stats["cancelled"]
                    })
                
                # å®šæœŸä¿å­˜è¿›åº¦
                if (stats["completed"] + stats["failed"]) % 10 == 0:
                    all_completed = completed_accessions | completed_in_session
                    save_progress_state(base_path, all_completed, failed_in_session)
        
        except KeyboardInterrupt:
            print_warning("\næ£€æµ‹åˆ°é”®ç›˜ä¸­æ–­ï¼Œæ­£åœ¨åœæ­¢ä¸‹è½½...")
            SHUTDOWN_EVENT.set()
            
        finally:
            if pbar:
                pbar.close()
            
            # æœ€ç»ˆä¿å­˜è¿›åº¦
            all_completed = completed_accessions | completed_in_session
            save_progress_state(base_path, all_completed, failed_in_session)
    
    # æ‰“å°æœ€ç»ˆç»Ÿè®¡ä¿¡æ¯
    elapsed = time.time() - stats["start_time"]
    print_info("\n" + "="*50)
    print_info("ä¸‹è½½å®Œæˆç»Ÿè®¡:")
    print_success(f"  æ€»è®¡: {stats['total']} ä¸ªæ–‡ä»¶")
    print_success(f"  æˆåŠŸ: {stats['completed']} ä¸ª")
    if stats["skipped"] > 0:
        print_info(f"  è·³è¿‡: {stats['skipped']} ä¸ª (å·²å­˜åœ¨)")
    if stats["failed"] > 0:
        print_error(f"  å¤±è´¥: {stats['failed']} ä¸ª")
    if stats["cancelled"] > 0:
        print_warning(f"  å–æ¶ˆ: {stats['cancelled']} ä¸ª")
    print_info(f"  ç”¨æ—¶: {elapsed:.1f} ç§’")
    if stats["total_size"] > 0:
        size_mb = stats["total_size"] / (1024 * 1024)
        print_info(f"  ä¸‹è½½å¤§å°: {size_mb:.2f} MB")
    print_info("="*50)
    
    return stats

def main():
    global MIN_INTERVAL, HAS_COLORAMA
    args = parse_args()

    # ç¦ç”¨å½©è‰²è¾“å‡ºå¦‚æœç”¨æˆ·æŒ‡å®š
    if args.no_color:
        HAS_COLORAMA = False

    # è®¾ç½®ä¿¡å·å¤„ç†å™¨
    signal.signal(signal.SIGINT, signal_handler)
    signal.signal(signal.SIGTERM, signal_handler)

    BASE_PATH = args.base_path.rstrip("/") + "/"
    FILE_PATH = args.file_path
    EMAIL = args.email
    API_KEY = args.api_key
    WORKERS = max(1, args.workers)
    RESUME = args.resume
    FORCE_REDOWNLOAD = args.force_redownload

    # å¦‚æœå¼ºåˆ¶é‡æ–°ä¸‹è½½ï¼Œåˆ™ç¦ç”¨æ–­ç‚¹ç»­è·‘
    if FORCE_REDOWNLOAD:
        RESUME = False
        print_warning("å¯ç”¨å¼ºåˆ¶é‡æ–°ä¸‹è½½æ¨¡å¼ï¼Œå°†è¦†ç›–æ‰€æœ‰ç°æœ‰æ–‡ä»¶")

    OUTPUT_FILE  = os.path.join(BASE_PATH, "generated_urls.txt")
    SUCCESS_FILE = os.path.join(BASE_PATH, "success.txt")
    FAILURE_FILE = os.path.join(BASE_PATH, "failure.txt")

    os.makedirs(BASE_PATH, exist_ok=True)

    # æ‰“å°å¯åŠ¨ä¿¡æ¯
    print_info("="*60)
    print_info("NCBI Assembly æ‰¹é‡ä¸‹è½½å·¥å…· (å¢å¼ºç‰ˆ)")
    print_info("="*60)
    print_info(f"è¾“å‡ºç›®å½•: {BASE_PATH}")
    print_info(f"è¾“å…¥æ–‡ä»¶: {FILE_PATH}")
    print_info(f"å¹¶å‘çº¿ç¨‹: {WORKERS}")
    print_info(f"æ–­ç‚¹ç»­è·‘: {'å¯ç”¨' if RESUME else 'ç¦ç”¨'}")
    print_info(f"å½©è‰²è¾“å‡º: {'å¯ç”¨' if HAS_COLORAMA else 'ç¦ç”¨'}")
    if HAS_TQDM:
        print_info("è¿›åº¦æ¡: å¯ç”¨")
    print_info("-"*60)

    # æ ¹æ®æ˜¯å¦æœ‰ API Key è®¾ç½®å…è®¸çš„å…¨å±€ rps
    if API_KEY:
        ALLOWED_RPS = 10.0   # å¸¦ key çš„å»ºè®®é€Ÿç‡
        print_info("æ£€æµ‹åˆ° API Keyï¼Œä½¿ç”¨è¾ƒé«˜è¯·æ±‚é¢‘ç‡")
    else:
        ALLOWED_RPS = 3.0    # æ—  key æ›´ä¿å®ˆ
        print_warning("æœªæä¾› API Keyï¼Œä½¿ç”¨ä¿å®ˆè¯·æ±‚é¢‘ç‡")
    
    # è®¡ç®—æœ€å°é—´éš”ï¼ˆå¤šä¸ªå¹¶å‘çº¿ç¨‹å…±äº«ï¼‰
    MIN_INTERVAL = float(WORKERS) / float(ALLOWED_RPS)
    print_info(f"è¯·æ±‚é—´éš”: {MIN_INTERVAL:.2f}ç§’ (ç›®æ ‡é¢‘ç‡: {ALLOWED_RPS} req/s)")

    try:
        # 1. è¯»å– accession åˆ—è¡¨ï¼Œå¹¶ç”Ÿæˆ URL
        print_progress("æ­£åœ¨è¯»å– accession åˆ—è¡¨...")
        if not os.path.exists(FILE_PATH):
            print_error(f"è¾“å…¥æ–‡ä»¶ä¸å­˜åœ¨: {FILE_PATH}")
            return 1
            
        accession_numbers = read_accession_file(FILE_PATH)
        if not accession_numbers:
            print_error("è¾“å…¥æ–‡ä»¶ä¸ºç©ºæˆ–æ²¡æœ‰æœ‰æ•ˆçš„ accession")
            return 1
            
        print_success(f"æˆåŠŸè¯»å– {len(accession_numbers)} ä¸ª accession")
        
        urls = generate_urls(accession_numbers)
        urls_and_accessions = list(zip(urls, accession_numbers))

        # 2. ä¿å­˜ URL åˆ—è¡¨
        print_progress("æ­£åœ¨ä¿å­˜ä¸‹è½½é“¾æ¥...")
        with open(OUTPUT_FILE, 'w', encoding='utf-8') as outf:
            outf.write(f"# Generated on {datetime.now().isoformat()}\n")
            outf.write(f"# Total URLs: {len(urls)}\n")
            for u in urls:
                outf.write(u + "\n")
        print_success(f"ä¸‹è½½é“¾æ¥å·²ä¿å­˜è‡³: {OUTPUT_FILE}")

        # 3. æ¸…ç†ä¹‹å‰çš„ä¸´æ—¶æ–‡ä»¶ (å¦‚æœä¸æ˜¯æ–­ç‚¹ç»­è·‘æ¨¡å¼)
        if not RESUME or FORCE_REDOWNLOAD:
            print_progress("æ­£åœ¨æ¸…ç†æ—§çš„ä¸´æ—¶æ–‡ä»¶...")
            temp_pattern = os.path.join(BASE_PATH, "*.part")
            temp_files = glob.glob(temp_pattern)
            for temp_file in temp_files:
                try:
                    os.remove(temp_file)
                except:
                    pass
            if temp_files:
                print_info(f"æ¸…ç†äº† {len(temp_files)} ä¸ªä¸´æ—¶æ–‡ä»¶")

        # 4. å¹¶å‘ä¸‹è½½
        print_progress("å¼€å§‹ä¸‹è½½...")
        stats = concurrent_download(
            urls_and_accessions, BASE_PATH, SUCCESS_FILE, FAILURE_FILE, 
            API_KEY, WORKERS, EMAIL, RESUME
        )
        
        # 5. æœ€ç»ˆæ¸…ç†å’Œæ€»ç»“
        cleanup_temp_files()
        
        if stats["failed"] == 0 and stats["cancelled"] == 0:
            print_success("ğŸ‰ æ‰€æœ‰æ–‡ä»¶ä¸‹è½½å®Œæˆ!")
        elif stats["completed"] > 0:
            print_warning(f"âš ï¸  éƒ¨åˆ†æ–‡ä»¶ä¸‹è½½å®Œæˆ: {stats['completed']}/{stats['total']}")
        else:
            print_error("âŒ æ²¡æœ‰æ–‡ä»¶æˆåŠŸä¸‹è½½")
            
        print_info(f"è¯¦ç»†æ—¥å¿—: æˆåŠŸ={SUCCESS_FILE}, å¤±è´¥={FAILURE_FILE}")
        return 0 if stats["failed"] == 0 else 1
        
    except KeyboardInterrupt:
        print_warning("\nç”¨æˆ·ä¸­æ–­æ“ä½œ")
        cleanup_temp_files()
        return 130
    except Exception as e:
        print_error(f"ç¨‹åºå¼‚å¸¸: {str(e)}")
        cleanup_temp_files()
        return 1

if __name__ == "__main__":
    try:
        exit_code = main()
        sys.exit(exit_code)
    except Exception as e:
        print_error(f"è‡´å‘½é”™è¯¯: {str(e)}")
        cleanup_temp_files()
        sys.exit(1)
